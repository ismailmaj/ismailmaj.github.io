{"componentChunkName":"component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx","path":"/tinkering-with-fizz-buzz-and-concurrency","result":{"data":{"post":{"slug":"/tinkering-with-fizz-buzz-and-concurrency","title":"Tinkering with fizz buzz and concurrency","date":"20.12.2022","tags":[{"name":"Rust","slug":"rust"},{"name":"Performance","slug":"performance"},{"name":"Concurrency","slug":"concurrency"}],"description":"Tinkering with fizz buzz and concurrency","canonicalUrl":null,"body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Tinkering with fizz buzz and concurrency\",\n  \"date\": \"2022-12-20T00:00:00.000Z\",\n  \"description\": \"Tinkering with fizz buzz and concurrency\",\n  \"tags\": [\"Rust\", \"Performance\", \"Concurrency\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null), mdx(\"p\", null, \"Let's get a baseline of the performance we could expect with a naive implementation of fizz buzz in Python.  \"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"i = 1\\nwhile True:\\n    if i % 15 == 0:\\n        print(\\\"FizzBuzz\\\")\\n    elif i % 3 == 0:\\n        print(\\\"Fizz\\\")\\n    elif i % 5 == 0:\\n        print(\\\"Buzz\\\")\\n    else:\\n        print(i)\\n    i += 1\\n\")), mdx(\"p\", null, \"using \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"pv\"), \" and redirecting to \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"/dev/null\"), \" we reach a whopping 30 MiB/s.  \"), mdx(\"p\", null, \"For the upper bound, we'll use the GNU utility \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"yes\"), \" known for its high throughput, it just prints lines of y.    \"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \">> yes           | pv --average-rate >/dev/null\\n[10.6GiB/s]\\n>> cat /dev/zero | pv --average-rate >/dev/null\\n[9.11GiB/s]\\n\")), mdx(\"p\", null, \"Nice.  \"), mdx(\"p\", null, \"Let's try to reproduce this command using Rust.  \"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-rust\"\n  }, \"use std::io::{self, stdout, Write};\\n\\nfn main() -> Result<(), io::Error> {\\n    let mut writer = stdout().lock();\\n    loop {\\n        writer.write(b\\\"y\\\\n\\\")?;\\n    }\\n}\\n\")), mdx(\"p\", null, \"And we reach a throughput of ... 10 MiB/s, 3 orders of magnitude slower.      \"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"How come Rust is slower than Python? Did I just waste my time learni-  \")), mdx(\"p\", null, \"No calm down, when running this code through \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"strace\"), \" we find out that we invoke the syscall \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"write\"), \" for each iteration of the loop. Reaching for the kernel is a considerable overhead and is our bottleneck so far.    \"), mdx(\"p\", null, \"The solution to this problem is buffering and it is what Python does by default.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"We can achieve the same in Rust by changing the writer from \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"Stdout\"), \" to a wrapper \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"BufWriter\"), \".\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"It writes to a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"Vec<u8>\"), \" in memory first and flushes to the underlying writer when necessary.    \"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-diff\"\n  }, \"- let mut writer = stdout().lock();\\n+ let mut writer = BufWriter::new(stdout().lock());\\n\")), mdx(\"p\", null, \"1.67GiB/s, great but still far from the 10GiB/s of the GNU utility.  \"), mdx(\"p\", null, \"The bottleneck now is the payload. For each function call we write 2 bytes of data which -- uhh isn't great.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"The CPU atomic operation on memory is a cache line, which is typically 64 bytes on x86/x64 CPUs.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"Let's write the payload 32 times and see if we improve.  \"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-rust\"\n  }, \"use std::io::{self, stdout, BufWriter};\\nuse std::io::prelude::*;\\n\\nconst PAGE_SIZE: usize = 4096;\\n\\nfn main() -> Result<(), io::Error> {\\n    let mut writer = BufWriter::with_capacity(PAGE_SIZE * 4, stdout().lock());\\n    loop {\\n        writer.write_all(b\\\"y\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\ny\\\\n\\\")?;\\n    }\\n}\\n\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \">> cargo run --release | pv --average-rate >/dev/null\\n[12.2GiB/s]\\n\")), mdx(\"p\", null, \"Nice! This is better than what I expected.\"), mdx(\"p\", null, \"Let's test our theory and write 66 bytes instead of 64.   \"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \">> cargo run --release | pv --average-rate >/dev/null\\n[9.6GiB/s]\\n\")), mdx(\"p\", null, \"A 27% difference.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"It's also beneficial to maintain a buffer that is a multiple of a page size as it's friendlier to Linux's ring buffer.        \"), mdx(\"p\", null, \"Let's try to transfer what we learned so far to write a performant fizz buzz implementation.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"As a first optimization, we can notice that the problem is cyclic and can be made branchless.  \"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-rust\"\n  }, \"use std::io::prelude::*;\\nuse std::io::{self, stdout, BufWriter};\\nconst PAGE_SIZE: usize = 4096;\\n\\nfn main() -> Result<(), io::Error> {\\n    let mut writer = BufWriter::with_capacity(PAGE_SIZE * 4, stdout().lock());\\n    for i in (0..).step_by(15) {\\n        write!(\\n            writer,\\n            \\\"{}\\\\n{}\\\\nFizz\\\\n{}\\\\nBuzz\\\\n{}\\\\n{}\\\\n{}\\\\nFizz\\\\nBuzz\\\\n{}\\\\nFizz\\\\n{}\\\\n{}\\\\nFizzBuzz\\\\n\\\",\\n            i + 1,\\n            i + 2,\\n            i + 4,\\n            i + 6,\\n            i + 7,\\n            i + 8,\\n            i + 11,\\n            i + 13,\\n            i + 14,\\n        )?;\\n    }\\n    Ok(())\\n}\\n\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \">> cargo run --release | pv --average-rate >/dev/null\\n[ 825MiB/s]\\n\")), mdx(\"p\", null, \"We are an order of magnitude off.  \"), mdx(\"p\", null, \"When running the following code on a perf flame graph, we can notice that two third of the time is spent on integer to string conversion or \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"itoa\"), \".   \"), mdx(\"p\", null, \"We are clearly CPU bound now.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"This can be solved by either multi-core parallelism or low-level optimization of the \\\"hot\\\" loop.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"We'll start by the latter.   \"), mdx(\"p\", null, \"We'll use the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"itoap\"), \" crate as it's one of the fastest simd itoa implementation based on \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"sse2\"), \".\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"It also writes numbers from the most significant digit first making the whole operation sequential.  \"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-rust\"\n  }, \"use std::io::prelude::*;\\nuse std::io::{self, stdout};\\n\\nuse itoap::write_to_vec;\\n\\nconst PAGE_SIZE: usize = 4096;\\nconst CHUNK_SIZE: usize = 15; // write 15 elements at a time\\nconst CHUNK_MAX_BYTES: usize = 359; // 39 * 8 u64 + 47 bytes of text\\n\\n// 128 KiB fits in the L2 cache of most processors\\nconst BUFFER_SIZE: usize = PAGE_SIZE * 32;\\n\\nconst BATCH_PER_PAGE: usize = PAGE_SIZE / CHUNK_MAX_BYTES;\\nconst BATCH_SIZE: usize = BATCH_PER_PAGE * 32;\\n\\nfn main() -> Result<(), io::Error> {\\n    let mut writer = stdout().lock();\\n    let mut i = 0;\\n    loop {\\n        let mut buf = Vec::<u8>::with_capacity(BUFFER_SIZE);\\n        (0..BATCH_SIZE).for_each(|_| {\\n            write_chunk(&mut buf, i);\\n            i += CHUNK_SIZE;\\n        });\\n        writer.write_all(&buf)?;\\n    }\\n}\\n\\n#[inline(always)]\\nfn write_chunk(buf: &mut Vec<u8>, i: usize) {\\n    write_to_vec(buf, i + 1);\\n    buf.push(b'\\\\n');\\n    write_to_vec(buf, i + 2);\\n    buf.extend(b\\\"\\\\nFizz\\\\n\\\");\\n    write_to_vec(buf, i + 4);\\n    buf.extend(b\\\"\\\\nBuzz\\\\nFizz\\\\n\\\");\\n    write_to_vec(buf, i + 7);\\n    buf.push(b'\\\\n');\\n    write_to_vec(buf, i + 8);\\n    buf.extend(b\\\"\\\\nFizz\\\\nBuzz\\\\n\\\");\\n    write_to_vec(buf, i + 11);\\n    buf.extend(b\\\"\\\\nFizz\\\\n\\\");\\n    write_to_vec(buf, i + 13);\\n    buf.push(b'\\\\n');\\n    write_to_vec(buf, i + 14);\\n    buf.extend(b\\\"\\\\nFizzBuzz\\\\n\\\");\\n}\\n\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \">> cargo run --release | pv --average-rate >/dev/null\\n[3.56GiB/s]\\n\")), mdx(\"p\", null, \"Not too bad for safe Rust that is still readable.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"Amusingly, reusing buffers didn't have any impact on the throughput.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"I'm guessing the allocator is smart enough to reuse buffers with the same capacity.  \"), mdx(\"p\", null, \"Now that we got reasonable single threaded performance we could use all the cores of our processor.  \"), mdx(\"p\", null, \"Since we know that we compute \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"CHUNK_SIZE * BATCH_SIZE = x\"), \" elements per loop, we could make each thread compute x elements at an offset i.e.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"thread 1: from 0 to x   \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"thread 2: from x to 2x\", mdx(\"br\", {\n    parentName: \"li\"\n  }), \"... \")), mdx(\"p\", null, \"We'll end up with as many buffers as threads that will produce each x elements in parallel!\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"We could then write all the buffers in a single syscall thanks to \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Vectored_I/O\"\n  }, \"vectored I/O\"), \".   \"), mdx(\"p\", null, \"The last thing we need to deal with is inter-thread communication.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"We'll use bounded channels to communicate buffers as we want to block when a worker is outpacing I/O.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"We will also make sure to manually \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"thread::yield_now()\"), \" to help the OS scheduler distribute the load.  \"), mdx(\"p\", null, \"The following program reaches a throughput of 6GiB/s on my 5950x CPU and 3733 MHz DDR4 RAM.  \"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-rust\"\n  }, \"#![feature(write_all_vectored)]\\nuse std::io;\\nuse std::io::prelude::*;\\nuse std::sync::mpsc::sync_channel;\\nuse std::thread::{self, available_parallelism};\\n\\nuse itoap::write_to_vec;\\n\\nconst PAGE_SIZE: usize = 4096;\\nconst CHUNK_SIZE: usize = 15;\\nconst CHUNK_MAX_BYTES: usize = 359;\\n\\n// limit the progression of a single worker\\nconst CHANNEL_CAPACITY: usize = 3;\\n\\nconst BUFFER_SIZE: usize = PAGE_SIZE * 128; // < L2 cache on 5950x\\nconst BATCH_PER_PAGE: usize = PAGE_SIZE / CHUNK_MAX_BYTES;\\nconst BATCH_SIZE: usize = BATCH_PER_PAGE * 128;\\n\\nfn main() -> Result<(), io::Error> {\\n    let workers_count = available_parallelism()?.get() - 1;\\n\\n    let receivers = (0..workers_count)\\n        .map(|thread_id| {\\n            let (sender, receiver) = sync_channel(CHANNEL_CAPACITY);\\n            thread::spawn(move || {\\n                // offset for this thread\\n                let mut i = thread_id * (CHUNK_SIZE * BATCH_SIZE);\\n                loop {\\n                    let mut buf = Vec::<u8>::with_capacity(BUFFER_SIZE);\\n                    (0..BATCH_SIZE).for_each(|_| {\\n                        write_chunk(&mut buf, i);\\n                        i += CHUNK_SIZE;\\n                    });\\n\\n                    sender\\n                        .send(buf)\\n                        .unwrap_or_else(|_| \\n                            panic!(\\\"thread {} couldn't send\\\", thread_id)\\n                        );\\n\\n                    // skip work done by other workers\\n                    i += (workers_count - 1) * (CHUNK_SIZE * BATCH_SIZE);\\n\\n                    thread::yield_now();\\n                }\\n            });\\n            receiver\\n        })\\n        .collect::<Vec<_>>();\\n\\n    let mut writer = io::stdout().lock();\\n    loop {\\n        let mut buffers = receivers\\n            .iter()\\n            .enumerate()\\n            .map(|(i, r)| {\\n                r.recv()\\n                 .unwrap_or_else(|_| panic!(\\\"worker thread {} died\\\", i))\\n            })\\n            .collect::<Vec<_>>();\\n\\n        writer.write_all_vectored(\\n            &mut buffers\\n                .iter_mut()\\n                .map(|b| io::IoSlice::new(b))\\n                .collect::<Vec<_>>(),\\n        )?;\\n    }\\n}\\n\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \">> cargo run --release | pv --average-rate >/dev/null\\n[6.07GiB/s]\\n\")), mdx(\"p\", null, \"It\\u2019s possible to go even further, but that either requires using obscure zero copy system calls or a more complex algorithm that exploits the ever increasing property of fizz buzz, \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"itoa\"), \" could then be specialized to serialize monotonically increasing numbers which, for most chunks, only recomputes the 2 least significant digits.      \"));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"Let's get a baseline of the performance we could expect with a naive implementation of fizz buzz in Python.   using  pv  and redirecting to…","timeToRead":2,"banner":null}},"pageContext":{"slug":"/tinkering-with-fizz-buzz-and-concurrency","formatString":"DD.MM.YYYY"}},"staticQueryHashes":["2744905544","3090400250","318001574"]}